{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup\n",
    "\n",
    "Let's import the required libraries and set up global variables for the rest of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): tqdm in c:\\anaconda\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 7.0.3, however version 7.1.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "!pip install tqdm\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import zipfile\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from lxml import objectify\n",
    "import codecs\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tarfile\n",
    "import subprocess\n",
    "import platform\n",
    "import time\n",
    "from tqdm import tqdm as progressbar # pandas df usage: 'for row in progressbar(df.itertuples(), total=df.shape[0])'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to create a directory under the specified path, gracefully handling errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __mkdir(*args):\n",
    "    path = os.path.join(*args)\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Study\\CS102\\project\\project2\\repro\\CS109Project\n"
     ]
    }
   ],
   "source": [
    "# Create the project directory holding the downloaded data, serialized dataframes and MetaMap install.\n",
    "# working_dir = __mkdir(os.path.expanduser(\"~\"), \"Medframes\")\n",
    "\n",
    "# Set working directory as the current directory of the ipython notebook\n",
    "working_dir = os.getcwd()\n",
    "\n",
    "print(\"Working directory: %s\" % working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download\n",
    "\n",
    "Download CSV data from clinicaltrials.gov. The data will be written in the working directory specified above as  [data_dir]/study_fields.csv.\n",
    "\n",
    "For clinicaltrials.gov, a search term needs to be specified. In this example, we'll download search results for the term \"seizure\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_ctgov(dest_dir, search_term):\n",
    "    print(\"Downloading clinicaltrials.gov results for '%s' to %s\" % (search_term, dest_dir))\n",
    "    dl_url = \"https://clinicaltrials.gov/ct2/results/download?down_stds=all&down_typ=results&down_flds=all&down_fmt=xml&term=%s&show_down=Y\" % search_term\n",
    "\n",
    "    # Download the zipped data and extract it to the output directory\n",
    "    out_path = os.path.join(dest_dir, \"download_ctgov.zip\")\n",
    "    with open(out_path, 'wb') as fh:\n",
    "        r = requests.get(dl_url)\n",
    "        for block in r.iter_content(1024):\n",
    "            fh.write(block)\n",
    "    with zipfile.ZipFile(out_path, 'r') as z:\n",
    "        z.extractall(dest_dir)\n",
    "    return dest_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_dir = __mkdir(working_dir, \"download\")\n",
    "download_ctgov(download_dir, \"seizure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas import\n",
    "\n",
    "Convert the downloaded CSV data to Pandas dataframes and serialize them as Python pickles. The function reads XML files from the working directory and writes to \"ctgov.pckl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ctgov_to_dataframe(src_dir):\n",
    "    # Get all XML files in the data directory\n",
    "    print(\"Transforming cliniclatrials download (%s) to dataframe\" % (src_dir))\n",
    "    data = []\n",
    "    for f in [_ for _ in os.listdir(src_dir) if _.endswith('.xml')]:\n",
    "        xml = objectify.parse(os.path.join(src_dir, f))\n",
    "        root = xml.getroot()\n",
    "        d = defaultdict(list)\n",
    "        for t in root.iter():\n",
    "            if t.text:\n",
    "                key = re.sub(r'\\[\\d+\\]', '', xml.getpath(t)).replace('/clinical_study/', '').replace('/', '.')\n",
    "                val = t.text.strip()\n",
    "                d[key].append(val)\n",
    "        d = {k: v[0] if len(v) == 1 else v for k, v in d.items()}\n",
    "        data.append(d)\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing dataframes\n",
    "Transform the downloaded data to Pandas dataframes and seialize them as Pytohn pickles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming cliniclatrials download (C:\\Study\\CS102\\project\\project2\\repro\\CS109Project\\download) to dataframe\n"
     ]
    }
   ],
   "source": [
    "download_dir = __mkdir(working_dir, \"download\")\n",
    "data_dir = __mkdir(working_dir, \"data\")\n",
    "ct_df = ctgov_to_dataframe(download_dir)\n",
    "ct_df.to_pickle(os.path.join(data_dir, 'ctgov.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataframes\n",
    "\n",
    "Read the pickled data back into Pandas and display the first 5 records. In this example, the pickled dataframe is serialized to \"ctgov.pckl\" in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__mkdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2edc9bde7215>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__mkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mctgov_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ctgov.pckl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mctgov_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__mkdir' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = __mkdir(working_dir, \"data\")\n",
    "ctgov_data = pd.read_pickle(os.path.join(data_dir, 'ctgov.pckl'))\n",
    "ctgov_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract criteria\n",
    "\n",
    "Read inthe serialized data from clinicaltrials.gov and extract inclusion/exlcusion criteria, one per row. Output a Series(id_info.nct_id, Criteria, Inclusion, TokenCount)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def id_generator(first_val=0, inc_func=lambda val: val + 1):\n",
    "    \"\"\"\n",
    "        Simple id generator. It takes first val & increase function and yields ids as needed.\n",
    "        Will return integers starting from 0 by default.\n",
    "    \"\"\"\n",
    "    id = first_val\n",
    "    while True:\n",
    "        yield id\n",
    "        id = inc_func(id)\n",
    "        \n",
    "        \n",
    "def __process_criteria(data, get_criteria_id):\n",
    "    pat = r\"^([\\w\\-]*\\s*){0,5}%s criteria[\\s\\w\\(\\),]*:\"\n",
    "    inpat = re.compile(pat % 'inclusion', re.UNICODE)\n",
    "    expat = re.compile(pat % 'exclusion', re.UNICODE)\n",
    "    try:\n",
    "        incl = True\n",
    "        nct_id = data[1]\n",
    "        txt = [_.strip() for _ in data[2].split(u'\\n\\n')]\n",
    "        for l in txt:\n",
    "            if re.match(inpat, l.lower()):\n",
    "                incl = True\n",
    "            elif re.match(expat, l.lower()):\n",
    "                incl = False\n",
    "            else:\n",
    "                toks = nltk.word_tokenize(l)\n",
    "                cri_id = next(get_criteria_id)\n",
    "                s = {'criteria_id': cri_id, 'NctId': nct_id, 'Criteria': unicode(l), 'Include': incl, 'Tokens': toks, 'TokenCount': len(toks)}\n",
    "                yield s\n",
    "    except Exception as e:\n",
    "        print(\"Error processing row %s: %s\" % (data[2], e))\n",
    "\n",
    "        \n",
    "def extract_criteria(data):\n",
    "    print(\"Transforming data (extracting criteria)\")\n",
    "    criteria_id_generator = id_generator()\n",
    "    transformed = [s for row in data[['id_info.nct_id', 'eligibility.criteria.textblock']].itertuples() for s in\n",
    "                   __process_criteria(row, criteria_id_generator)]\n",
    "    df = pd.DataFrame(transformed)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data and write the result to a file. (You'll notice that the script logs an error for one row. This is expected and results from that row being a \"NaN\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data (extracting criteria)\n",
      "Error processing row nan: 'float' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "data_dir = __mkdir(working_dir, \"data\")\n",
    "ctgov_data = pd.read_pickle(os.path.join(data_dir, 'ctgov.pckl'))\n",
    "# Extract criteria\n",
    "criteria = extract_criteria(ctgov_data)\n",
    "criteria.to_pickle(os.path.join(data_dir, 'ct_criteria.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back the data and display a record selected by column value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Criteria</th>\n",
       "      <th>Include</th>\n",
       "      <th>NctId</th>\n",
       "      <th>TokenCount</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>criteria_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patients.</td>\n",
       "      <td>True</td>\n",
       "      <td>NCT00001192</td>\n",
       "      <td>2</td>\n",
       "      <td>[Patients, .]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Criteria Include        NctId  TokenCount         Tokens  criteria_id\n",
       "4  Patients.    True  NCT00001192           2  [Patients, .]            4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria = pd.read_pickle(os.path.join(data_dir, 'ct_criteria.pckl'))\n",
    "criteria.loc[criteria['criteria_id'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Criteria</th>\n",
       "      <th>Include</th>\n",
       "      <th>NctId</th>\n",
       "      <th>TokenCount</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>criteria_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9611</th>\n",
       "      <td>1. Diagnosis of Partial/Focal Onset Epilepsy (...</td>\n",
       "      <td>True</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>13</td>\n",
       "      <td>[1, ., Diagnosis, of, Partial/Focal, Onset, Ep...</td>\n",
       "      <td>9611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9612</th>\n",
       "      <td>2. Ages 18-70</td>\n",
       "      <td>True</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>4</td>\n",
       "      <td>[2, ., Ages, 18-70]</td>\n",
       "      <td>9612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9613</th>\n",
       "      <td>1. Pregnancy</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, ., Pregnancy]</td>\n",
       "      <td>9613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9614</th>\n",
       "      <td>2. Recent trauma such as motor vehicle acciden...</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>15</td>\n",
       "      <td>[2, ., Recent, trauma, such, as, motor, vehicl...</td>\n",
       "      <td>9614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9615</th>\n",
       "      <td>3. Currently on medication, other than for epi...</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>22</td>\n",
       "      <td>[3, ., Currently, on, medication, ,, other, th...</td>\n",
       "      <td>9615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9616</th>\n",
       "      <td>4. If diagnosed with a condition which could a...</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>15</td>\n",
       "      <td>[4, ., If, diagnosed, with, a, condition, whic...</td>\n",
       "      <td>9616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9617</th>\n",
       "      <td>1. Irritable bowel syndrome</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, ., Irritable, bowel, syndrome]</td>\n",
       "      <td>9617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9618</th>\n",
       "      <td>2. Crohn's disease</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>5</td>\n",
       "      <td>[2, ., Crohn, 's, disease]</td>\n",
       "      <td>9618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9619</th>\n",
       "      <td>3. Ulcerative colitis</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>4</td>\n",
       "      <td>[3, ., Ulcerative, colitis]</td>\n",
       "      <td>9619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9620</th>\n",
       "      <td>4. Migraine headache with abdominal manifestation</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>7</td>\n",
       "      <td>[4, ., Migraine, headache, with, abdominal, ma...</td>\n",
       "      <td>9620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9621</th>\n",
       "      <td>1. No history or diagnosis of any seizure diso...</td>\n",
       "      <td>True</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>10</td>\n",
       "      <td>[1, ., No, history, or, diagnosis, of, any, se...</td>\n",
       "      <td>9621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9622</th>\n",
       "      <td>2. Ages 18-70</td>\n",
       "      <td>True</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>4</td>\n",
       "      <td>[2, ., Ages, 18-70]</td>\n",
       "      <td>9622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9623</th>\n",
       "      <td>1. Pregnancy</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, ., Pregnancy]</td>\n",
       "      <td>9623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9624</th>\n",
       "      <td>2. Recent trauma such as motor vehicle acciden...</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>15</td>\n",
       "      <td>[2, ., Recent, trauma, such, as, motor, vehicl...</td>\n",
       "      <td>9624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9625</th>\n",
       "      <td>3. Currently on medication, other than for epi...</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>22</td>\n",
       "      <td>[3, ., Currently, on, medication, ,, other, th...</td>\n",
       "      <td>9625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9626</th>\n",
       "      <td>4. If diagnosed with a condition which could a...</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>15</td>\n",
       "      <td>[4, ., If, diagnosed, with, a, condition, whic...</td>\n",
       "      <td>9626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9627</th>\n",
       "      <td>1. Irritable bowel syndrome</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, ., Irritable, bowel, syndrome]</td>\n",
       "      <td>9627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9628</th>\n",
       "      <td>2. Crohn's disease</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>5</td>\n",
       "      <td>[2, ., Crohn, 's, disease]</td>\n",
       "      <td>9628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9629</th>\n",
       "      <td>3. Ulcerative colitis</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>4</td>\n",
       "      <td>[3, ., Ulcerative, colitis]</td>\n",
       "      <td>9629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9630</th>\n",
       "      <td>4. Migraine headache with abdominal manifestation</td>\n",
       "      <td>False</td>\n",
       "      <td>NCT01373190</td>\n",
       "      <td>7</td>\n",
       "      <td>[4, ., Migraine, headache, with, abdominal, ma...</td>\n",
       "      <td>9630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Criteria Include        NctId  \\\n",
       "9611  1. Diagnosis of Partial/Focal Onset Epilepsy (...    True  NCT01373190   \n",
       "9612                                      2. Ages 18-70    True  NCT01373190   \n",
       "9613                                       1. Pregnancy   False  NCT01373190   \n",
       "9614  2. Recent trauma such as motor vehicle acciden...   False  NCT01373190   \n",
       "9615  3. Currently on medication, other than for epi...   False  NCT01373190   \n",
       "9616  4. If diagnosed with a condition which could a...   False  NCT01373190   \n",
       "9617                        1. Irritable bowel syndrome   False  NCT01373190   \n",
       "9618                                 2. Crohn's disease   False  NCT01373190   \n",
       "9619                              3. Ulcerative colitis   False  NCT01373190   \n",
       "9620  4. Migraine headache with abdominal manifestation   False  NCT01373190   \n",
       "9621  1. No history or diagnosis of any seizure diso...    True  NCT01373190   \n",
       "9622                                      2. Ages 18-70    True  NCT01373190   \n",
       "9623                                       1. Pregnancy   False  NCT01373190   \n",
       "9624  2. Recent trauma such as motor vehicle acciden...   False  NCT01373190   \n",
       "9625  3. Currently on medication, other than for epi...   False  NCT01373190   \n",
       "9626  4. If diagnosed with a condition which could a...   False  NCT01373190   \n",
       "9627                        1. Irritable bowel syndrome   False  NCT01373190   \n",
       "9628                                 2. Crohn's disease   False  NCT01373190   \n",
       "9629                              3. Ulcerative colitis   False  NCT01373190   \n",
       "9630  4. Migraine headache with abdominal manifestation   False  NCT01373190   \n",
       "\n",
       "      TokenCount                                             Tokens  \\\n",
       "9611          13  [1, ., Diagnosis, of, Partial/Focal, Onset, Ep...   \n",
       "9612           4                                [2, ., Ages, 18-70]   \n",
       "9613           3                                  [1, ., Pregnancy]   \n",
       "9614          15  [2, ., Recent, trauma, such, as, motor, vehicl...   \n",
       "9615          22  [3, ., Currently, on, medication, ,, other, th...   \n",
       "9616          15  [4, ., If, diagnosed, with, a, condition, whic...   \n",
       "9617           5                 [1, ., Irritable, bowel, syndrome]   \n",
       "9618           5                         [2, ., Crohn, 's, disease]   \n",
       "9619           4                        [3, ., Ulcerative, colitis]   \n",
       "9620           7  [4, ., Migraine, headache, with, abdominal, ma...   \n",
       "9621          10  [1, ., No, history, or, diagnosis, of, any, se...   \n",
       "9622           4                                [2, ., Ages, 18-70]   \n",
       "9623           3                                  [1, ., Pregnancy]   \n",
       "9624          15  [2, ., Recent, trauma, such, as, motor, vehicl...   \n",
       "9625          22  [3, ., Currently, on, medication, ,, other, th...   \n",
       "9626          15  [4, ., If, diagnosed, with, a, condition, whic...   \n",
       "9627           5                 [1, ., Irritable, bowel, syndrome]   \n",
       "9628           5                         [2, ., Crohn, 's, disease]   \n",
       "9629           4                        [3, ., Ulcerative, colitis]   \n",
       "9630           7  [4, ., Migraine, headache, with, abdominal, ma...   \n",
       "\n",
       "      criteria_id  \n",
       "9611         9611  \n",
       "9612         9612  \n",
       "9613         9613  \n",
       "9614         9614  \n",
       "9615         9615  \n",
       "9616         9616  \n",
       "9617         9617  \n",
       "9618         9618  \n",
       "9619         9619  \n",
       "9620         9620  \n",
       "9621         9621  \n",
       "9622         9622  \n",
       "9623         9623  \n",
       "9624         9624  \n",
       "9625         9625  \n",
       "9626         9626  \n",
       "9627         9627  \n",
       "9628         9628  \n",
       "9629         9629  \n",
       "9630         9630  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria.loc[criteria['NctId'] == 'NCT01373190']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag, lemmatize, ngrammize\n",
    "\n",
    "Processes the extracted criteria with the NLTK POS tagger and lemmatizer and generates ngrams of 1-3 words (note: while unigrams are technically duplicated as 'Tokens', it will be more convenient to allow this and keep them in one column with bigrams and trigrams). Preprocesses the tokens by removing special characters and punctuation. Lemmata and ngrams are lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __lemmatise(lemmatizer, r):\n",
    "    wn_tags = {'NN': nltk.corpus.wordnet.NOUN, 'JJ': nltk.corpus.wordnet.ADJ, 'VB': nltk.corpus.wordnet.VERB,\n",
    "               'RB': nltk.corpus.wordnet.ADV}\n",
    "    return [(t[0], lemmatizer.lemmatize(t[0].lower(), pos=wn_tags.get(t[1][:2], nltk.corpus.wordnet.NOUN)).lower()) for\n",
    "            t in r]\n",
    "\n",
    "\n",
    "def tag_and_stem(data):\n",
    "    print(\"Transforming data (tagging and lemmatising)\")\n",
    "    series = []\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    punct = '[%s]*' % re.escape(string.punctuation)\n",
    "    pat = re.compile(r\"^(%(p)s[\\w\\d]+%(p)s)+$\" % {'p': punct}, re.UNICODE)\n",
    "    # Itertuples is 50% faster than df.apply()\n",
    "    for row in progressbar(data[['NctId', 'Tokens', 'criteria_id']].itertuples(), total=data.shape[0]):\n",
    "        nct_id = row[1]\n",
    "        toks = filter(lambda t: re.match(pat, t), row[2])\n",
    "        cri_id = row[3]\n",
    "        tags = nltk.pos_tag(toks)\n",
    "        lemmas = __lemmatise(lemmatizer, tags)\n",
    "        ngrams = []\n",
    "        for n in (1, 2, 3):\n",
    "            ngrams += list(nltk.ngrams([(lemma[1], tags[idx][1]) for idx, lemma in enumerate(lemmas)], n))\n",
    "        s = {'criteria_id': cri_id, 'NctId': nct_id, 'Tokens': toks, 'Tags': tags, 'Lemmas': lemmas, 'Ngrams': ngrams}\n",
    "        series.append(s)\n",
    "    df = pd.DataFrame(series)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the extracted criteria (stored in \"ct_criteria.pckl\" in the previous step), tag, lemmatize and ngrammize the data and store it as \"ct_tagged.pckl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data (tagging and lemmatising)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "data_dir = __mkdir(working_dir, \"data\")\n",
    "\n",
    "criteria = pd.read_pickle(os.path.join(data_dir, 'ct_criteria.pckl'))\n",
    "tagged = tag_and_stem(criteria)\n",
    "tagged.to_pickle(os.path.join(data_dir, 'ct_tagged.pckl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>NctId</th>\n",
       "      <th>Ngrams</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>criteria_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(History, history), (of, of), (uncontrolled, ...</td>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>[((history, NN),), ((of, IN),), ((uncontrolled...</td>\n",
       "      <td>[(History, NN), (of, IN), (uncontrolled, VBN),...</td>\n",
       "      <td>[History, of, uncontrolled, seizures, at, the,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(Seizure, seizure), (frequency, frequency), (...</td>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>[((seizure, NN),), ((frequency, NN),), ((by, I...</td>\n",
       "      <td>[(Seizure, NN), (frequency, NN), (by, IN), (hi...</td>\n",
       "      <td>[Seizure, frequency, by, history, must, be, su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(Patients, patient), (of, of), (any, any), (a...</td>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>[((patient, NNS),), ((of, IN),), ((any, DT),),...</td>\n",
       "      <td>[(Patients, NNS), (of, IN), (any, DT), (age, N...</td>\n",
       "      <td>[Patients, of, any, age, may, be, accepted]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(Patients, patient), (and, and), (parents, pa...</td>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>[((patient, NNS),), ((and, CC),), ((parent, NN...</td>\n",
       "      <td>[(Patients, NNS), (and, CC), (parents, NNS), (...</td>\n",
       "      <td>[Patients, and, parents, or, guardians, if, ap...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Patients, patient)]</td>\n",
       "      <td>NCT00001192</td>\n",
       "      <td>[((patient, NNS),)]</td>\n",
       "      <td>[(Patients, NNS)]</td>\n",
       "      <td>[Patients]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Lemmas        NctId  \\\n",
       "0  [(History, history), (of, of), (uncontrolled, ...  NCT00001149   \n",
       "1  [(Seizure, seizure), (frequency, frequency), (...  NCT00001149   \n",
       "2  [(Patients, patient), (of, of), (any, any), (a...  NCT00001149   \n",
       "3  [(Patients, patient), (and, and), (parents, pa...  NCT00001149   \n",
       "4                              [(Patients, patient)]  NCT00001192   \n",
       "\n",
       "                                              Ngrams  \\\n",
       "0  [((history, NN),), ((of, IN),), ((uncontrolled...   \n",
       "1  [((seizure, NN),), ((frequency, NN),), ((by, I...   \n",
       "2  [((patient, NNS),), ((of, IN),), ((any, DT),),...   \n",
       "3  [((patient, NNS),), ((and, CC),), ((parent, NN...   \n",
       "4                                [((patient, NNS),)]   \n",
       "\n",
       "                                                Tags  \\\n",
       "0  [(History, NN), (of, IN), (uncontrolled, VBN),...   \n",
       "1  [(Seizure, NN), (frequency, NN), (by, IN), (hi...   \n",
       "2  [(Patients, NNS), (of, IN), (any, DT), (age, N...   \n",
       "3  [(Patients, NNS), (and, CC), (parents, NNS), (...   \n",
       "4                                  [(Patients, NNS)]   \n",
       "\n",
       "                                              Tokens  criteria_id  \n",
       "0  [History, of, uncontrolled, seizures, at, the,...            0  \n",
       "1  [Seizure, frequency, by, history, must, be, su...            1  \n",
       "2        [Patients, of, any, age, may, be, accepted]            2  \n",
       "3  [Patients, and, parents, or, guardians, if, ap...            3  \n",
       "4                                         [Patients]            4  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = pd.read_pickle(os.path.join(data_dir, 'ct_tagged.pckl'))\n",
    "tagged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter criteria\n",
    "Filters out criteria composed entirely of function words and stopwords. Strips ngrams composed entirely of stop words/tags from the ngram list. By default this function uses the NTLK stopword list and all PTB tags except nouns. Additional lists of stop words and stop tags can be supplied with keyword arguments (\"stop_words\", \"stop_tags\"). Returns a tuple of dataframes, (filtered_criteria, excluded_criteria).\n",
    "\n",
    "(Note: this step generates a SettingWithCopyWarning. This is known and is a false positive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __filter(values, idx, stops):\n",
    "    return not set([t[idx] for t in values]) <= stops\n",
    "\n",
    "\n",
    "def filter_criteria(data, user_stop_words=[], user_stop_tags=[]):\n",
    "    print(\"Filtering criteria\")\n",
    "    default_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    default_stop_tags = [\"$\", \"''\", \"(\", \")\", \",\", \"--\", \".\", \":\", \"CC\", \"CD\", \"DT\",\n",
    "                         \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\",\n",
    "                         \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\",\n",
    "                         \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\",\n",
    "                         \"WDT\", \"WP\", \"WP$\", \"WRB\", \"``\"]\n",
    "    print(\"Filtering stops\")\n",
    "    stop_words = set(default_stop_words + user_stop_words)\n",
    "    stop_tags = set(default_stop_tags + user_stop_tags)\n",
    "    excluded = pd.DataFrame()\n",
    "    for col, idx, stops in (\n",
    "            ('Lemmas', 0, stop_words),\n",
    "            ('Tags', 1, stop_tags)):  # Lemma filtering excludes 18 rows, tag filtering excludes 205\n",
    "        data['Ngrams'] = data['Ngrams'].apply(lambda row: [ngram for ngram in row if __filter(ngram, idx, stops)])\n",
    "        groups = data.groupby(lambda r: __filter(data[col].loc[r], 1, stops))\n",
    "        data = groups.get_group(True)\n",
    "        excluded = excluded.append(groups.get_group(False)) if groups.groups.has_key(False) else excluded\n",
    "    return (data, excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the tagged criteria (stored in \"ct_tagged.pckl\" in the previous step), filter out noise and write the results to \"ct_filtered.pckl\" (the included criteria) and \"ct_excluded.pckl\" (the excluded criteria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering criteria\n",
      "Filtering stops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data_dir = __mkdir(working_dir, \"data\")\n",
    "\n",
    "criteria = pd.read_pickle(os.path.join(data_dir, 'ct_tagged.pckl'))\n",
    "incl, excl = filter_criteria(criteria)\n",
    "incl.to_pickle(os.path.join(data_dir, 'ct_filtered.pckl'))\n",
    "excl.to_pickle(os.path.join(data_dir, 'ct_excluded.pckl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>NctId</th>\n",
       "      <th>Ngrams</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>criteria_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(History, history), (of, of), (uncontrolled, ...</td>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>[((history, NN),), ((seizure, NNS),), ((presen...</td>\n",
       "      <td>[(History, NN), (of, IN), (uncontrolled, VBN),...</td>\n",
       "      <td>[History, of, uncontrolled, seizures, at, the,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(Seizure, seizure), (frequency, frequency), (...</td>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>[((seizure, NN),), ((frequency, NN),), ((histo...</td>\n",
       "      <td>[(Seizure, NN), (frequency, NN), (by, IN), (hi...</td>\n",
       "      <td>[Seizure, frequency, by, history, must, be, su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(Patients, patient), (of, of), (any, any), (a...</td>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>[((patient, NNS),), ((age, NN),), ((patient, N...</td>\n",
       "      <td>[(Patients, NNS), (of, IN), (any, DT), (age, N...</td>\n",
       "      <td>[Patients, of, any, age, may, be, accepted]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(Patients, patient), (and, and), (parents, pa...</td>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>[((patient, NNS),), ((parent, NNS),), ((guardi...</td>\n",
       "      <td>[(Patients, NNS), (and, CC), (parents, NNS), (...</td>\n",
       "      <td>[Patients, and, parents, or, guardians, if, ap...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Patients, patient)]</td>\n",
       "      <td>NCT00001192</td>\n",
       "      <td>[((patient, NNS),)]</td>\n",
       "      <td>[(Patients, NNS)]</td>\n",
       "      <td>[Patients]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Lemmas        NctId  \\\n",
       "0  [(History, history), (of, of), (uncontrolled, ...  NCT00001149   \n",
       "1  [(Seizure, seizure), (frequency, frequency), (...  NCT00001149   \n",
       "2  [(Patients, patient), (of, of), (any, any), (a...  NCT00001149   \n",
       "3  [(Patients, patient), (and, and), (parents, pa...  NCT00001149   \n",
       "4                              [(Patients, patient)]  NCT00001192   \n",
       "\n",
       "                                              Ngrams  \\\n",
       "0  [((history, NN),), ((seizure, NNS),), ((presen...   \n",
       "1  [((seizure, NN),), ((frequency, NN),), ((histo...   \n",
       "2  [((patient, NNS),), ((age, NN),), ((patient, N...   \n",
       "3  [((patient, NNS),), ((parent, NNS),), ((guardi...   \n",
       "4                                [((patient, NNS),)]   \n",
       "\n",
       "                                                Tags  \\\n",
       "0  [(History, NN), (of, IN), (uncontrolled, VBN),...   \n",
       "1  [(Seizure, NN), (frequency, NN), (by, IN), (hi...   \n",
       "2  [(Patients, NNS), (of, IN), (any, DT), (age, N...   \n",
       "3  [(Patients, NNS), (and, CC), (parents, NNS), (...   \n",
       "4                                  [(Patients, NNS)]   \n",
       "\n",
       "                                              Tokens  criteria_id  \n",
       "0  [History, of, uncontrolled, seizures, at, the,...            0  \n",
       "1  [Seizure, frequency, by, history, must, be, su...            1  \n",
       "2        [Patients, of, any, age, may, be, accepted]            2  \n",
       "3  [Patients, and, parents, or, guardians, if, ap...            3  \n",
       "4                                         [Patients]            4  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incl = pd.read_pickle(os.path.join(data_dir, 'ct_filtered.pckl'))\n",
    "incl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>NctId</th>\n",
       "      <th>Ngrams</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>criteria_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>[(Other, other)]</td>\n",
       "      <td>NCT00004399</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(Other, JJ)]</td>\n",
       "      <td>[Other]</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>[(Other, other)]</td>\n",
       "      <td>NCT00047073</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(Other, JJ)]</td>\n",
       "      <td>[Other]</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>[(Other, other)]</td>\n",
       "      <td>NCT00047073</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(Other, JJ)]</td>\n",
       "      <td>[Other]</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>[(Other, other)]</td>\n",
       "      <td>NCT00068770</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(Other, JJ)]</td>\n",
       "      <td>[Other]</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>[(Other, other)]</td>\n",
       "      <td>NCT00068770</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(Other, JJ)]</td>\n",
       "      <td>[Other]</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Lemmas        NctId Ngrams           Tags   Tokens  criteria_id\n",
       "149  [(Other, other)]  NCT00004399     []  [(Other, JJ)]  [Other]          149\n",
       "610  [(Other, other)]  NCT00047073     []  [(Other, JJ)]  [Other]          610\n",
       "636  [(Other, other)]  NCT00047073     []  [(Other, JJ)]  [Other]          636\n",
       "959  [(Other, other)]  NCT00068770     []  [(Other, JJ)]  [Other]          959\n",
       "993  [(Other, other)]  NCT00068770     []  [(Other, JJ)]  [Other]          993"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excl = pd.read_pickle(os.path.join(data_dir, 'ct_excluded.pckl'))\n",
    "excl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MetaMap installation\n",
    "\n",
    "This will start the server, run a simple query in interactive mode and stop the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_metamap(mm_dir):\n",
    "    start_scripts = {\"Linux\": \"bin/skrmedpostctl start\",\n",
    "                     \"Windows\": \"bin\\skrmedpostctl_start.bat\",\n",
    "                     \"MacOS\": \"bin/skrmedpostctl start\"}\n",
    "    start_script = start_scripts[platform.system()]\n",
    "    os.chdir(mm_dir)\n",
    "    subprocess.Popen([start_script], shell=True)\n",
    "    # __execute(['bin/wsdserverctl','start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stop_metamap(mm_dir):\n",
    "    stop_scripts = {\"Linux\": \"bin/skrmedpostctl stop\",\n",
    "                    \"Windows\": \"bin\\skrmedpostctl_stop.bat\",\n",
    "                    \"MacOS\": \"bin/skrmedpostctl stop\"}\n",
    "    stop_script = stop_scripts[platform.system()]\n",
    "    os.chdir(mm_dir)\n",
    "    subprocess.Popen([stop_script], shell=True)\n",
    "    # __execute(['bin/skrmedpostctl', 'stop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_metamap(mm_dir):\n",
    "    mm_scripts = {\"Linux\": 'echo \"common flu\" | ./bin/metamap -I',\n",
    "                  \"Windows\": 'echo \"common flu\" | bin\\metamap.bat -I',\n",
    "                  \"MacOS\": 'echo \"common flu\" | ./bin/metamap -I'}\n",
    "    mm_script = mm_scripts[platform.system()]\n",
    "    os.chdir(mm_dir)\n",
    "    print subprocess.check_output(mm_script, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set here the path to the MetaMap installation folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mm_dir = \"C:\\\\Study\\\\CS102\\\\project\\\\public_mm_win32_main_2014\\\\public_mm\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "C:\\Study\\CS102\\project\\public_mm_win32_main_2014\\public_mm>set path=C:\\Anaconda\\lib\\site-packages\\numpy\\core;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\Program Files\\Common Files\\Microsoft Shared\\Windows Live;C:\\Program Files (x86)\\Common Files\\Microsoft Shared\\Windows Live;C:\\Program Files (x86)\\Intel\\iCLS Client\\;C:\\Program Files\\Intel\\iCLS Client\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Program Files (x86)\\Intel\\OpenCL SDK\\3.0\\bin\\x86;C:\\Program Files (x86)\\Intel\\OpenCL SDK\\3.0\\bin\\x64;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;c:\\Program Files (x86)\\Microsoft SQL Server\\100\\Tools\\Binn\\;c:\\Program Files\\Microsoft SQL Server\\100\\Tools\\Binn\\;c:\\Program Files\\Microsoft SQL Server\\100\\DTS\\Binn\\;C:\\Program Files (x86)\\Microsoft ASP.NET\\ASP.NET Web Pages\\v1.0\\;C:\\Program Files\\Microsoft SQL Server\\110\\Tools\\Binn\\;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\Windows Live\\Shared;C:\\Program Files\\Microsoft\\Web Platform Installer\\;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files\\Microsoft SQL Server\\120\\Tools\\Binn\\;C:\\Program Files (x86)\\Microsoft Team Foundation Server 2013 Power Tools\\;C:\\Program Files (x86)\\Microsoft Team Foundation Server 2013 Power Tools\\Best Practices Analyzer\\;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\1.0\\;C:\\Program Files (x86)\\Skype\\Phone\\;C:\\Anaconda;C:\\Anaconda\\Scripts;C:\\Program Files (x86)\\QuickTime\\QTSystem\\;C:\\Program Files\\Amazon\\AWSCLI\\;C:\\Program Files (x86)\\HMA! Pro VPN\\bin;C:\\Users\\alandschaft\\AppData\\Local\\Pandoc\\;C:\\Study\\CS102\\project\\public_mm_win32_main_2014\\public_mm\\bin \r\n",
      "\r\n",
      "C:\\Study\\CS102\\project\\public_mm_win32_main_2014\\public_mm>C:\\Study\\CS102\\project\\public_mm_win32_main_2014\\public_mm\\bin\\sh C:\\Study\\CS102\\project\\public_mm_win32_main_2014\\public_mm\\bin\\metamap14 -I \r\n",
      "C:/Study/CS102/project/public_mm_win32_main_2014/public_mm/bin/SKRrun.14 C:/Study/CS102/project/public_mm_win32_main_2014/public_mm/bin/metamap14.BINARY.x86-win32-nt-4 --lexicon db -Z 2014AA -I\n",
      "Berkeley DB databases (USAbase 2014AA strict model) are open.\r\n",
      "Static variants will come from table varsan in C:/Study/CS102/project/public_mm_win32_main_2014/public_mm/DB/DB.USAbase.2014AA.strict.\r\n",
      "Derivational Variants: Adj/noun ONLY.\r\n",
      "Variant generation mode: static.\r\n",
      "Established connection $stream(44654736) to TAGGER Server on localhost.\r\n",
      "\r\n",
      "metamap14.binary.x86-win32-nt-4 (2014)\r\n",
      "\r\n",
      "Control options:\r\n",
      "  composite_phrases=4\r\n",
      "  lexicon=db\r\n",
      "  mm_data_year=2014AA\r\n",
      "  show_cuis\r\n",
      "Processing 00000000.tx.1: \"common flu\"\r\n",
      "\r\n",
      "Phrase: \"common flu\"\r\n",
      "Meta Mapping (888):\r\n",
      "   694   C0205214:Common (Common (qualifier value)) [Quantitative Concept]\r\n",
      "   861   C2348686:FlU (Fluorescence Units) [Quantitative Concept]\r\n",
      "Meta Mapping (888):\r\n",
      "   694   C0205214:Common (Common (qualifier value)) [Quantitative Concept]\r\n",
      "   861   C0021400:Flu (Influenza) [Disease or Syndrome]\r\n",
      "Meta Mapping (888):\r\n",
      "   694   C3245511:common (Common Specifications in HL7 V3 Publishing) [Intellectual Product]\r\n",
      "   861   C2348686:FlU (Fluorescence Units) [Quantitative Concept]\r\n",
      "Meta Mapping (888):\r\n",
      "   694   C3245511:common (Common Specifications in HL7 V3 Publishing) [Intellectual Product]\r\n",
      "   861   C0021400:Flu (Influenza) [Disease or Syndrome]\r\n",
      "Meta Mapping (888):\r\n",
      "   694   C1522138:Common (shared attribute) [Functional Concept]\r\n",
      "   861   C2348686:FlU (Fluorescence Units) [Quantitative Concept]\r\n",
      "Meta Mapping (888):\r\n",
      "   694   C1522138:Common (shared attribute) [Functional Concept]\r\n",
      "   861   C0021400:Flu (Influenza) [Disease or Syndrome]\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_metamap(mm_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the data with MetaMap\n",
    "\n",
    "#### If you did not install metamap, skip to section \"Process MetaMap results\"\n",
    "\n",
    "Dump the ngrams to a text file in the \"list of terms with IDs\" format(id: \"{criteria id}-{ngram index in criteria}\") and tag it with MetaMap. Outputs the \"fielded NMI\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_mm_input(df, dest_file, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"Generating MetaMap input at %s\" % dest_file)\n",
    "    with codecs.open(dest_file, 'w', encoding='ascii', errors='ignore') as fh:\n",
    "        for r in df[['Ngrams', 'criteria_id']].itertuples():\n",
    "            ngrams = r[1]\n",
    "            cri_id = r[2]\n",
    "            for ngrami, ngram in enumerate(ngrams):\n",
    "                line = '-'.join((str(cri_id), str(ngrami))) +'|'+' '.join(_[0] for _ in ngram)\n",
    "                fh.write(line+os.linesep)\n",
    "    return dest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_metamap(mm_dir, src_file, dest_file, verbose=True):\n",
    "    try:\n",
    "        num_lines = sum(1 for line in open(src_file, 'r'))\n",
    "        total = 2*num_lines + 23\n",
    "        if verbose:\n",
    "            print(\"Running MetaMap on file %s, writing to %s\" % (src_file, dest_file))\n",
    "        mm_scripts = {\"Linux\": './bin/metamap --sldiID -z -i -N %s %s',\n",
    "                      \"Windows\": 'bin\\metamap.bat --sldiID -z -i -N %s %s',\n",
    "                      \"MacOS\": './bin/metamap -I --sldiID -z -i -N %s %s'}\n",
    "        mm_script = mm_scripts[platform.system()] % (src_file, dest_file)\n",
    "        os.chdir(mm_dir)\n",
    "        process = subprocess.Popen(mm_script, stdout=subprocess.PIPE, shell=True)\n",
    "        if verbose:\n",
    "            for line in progressbar(iter(process.stdout.readline, ''), total=total):\n",
    "#                 sys.stdout.write(line)\n",
    "                pass # no need for stdout if we use progressbar\n",
    "        else:\n",
    "            for line in iter(process.stdout.readline, ''):\n",
    "                pass\n",
    "        return dest_file\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_mm_serial(data):\n",
    "    start = time.time()\n",
    "\n",
    "    mm_in = prepare_mm_input(data, os.path.join(data_dir, 'mm_in.txt'))\n",
    "    m_out = run_metamap(mm_dir, mm_in, os.path.join(data_dir, 'mm_out.txt'))\n",
    "\n",
    "    print 'Done. Serial processing time: %.1f sec' % round(time.time() - start, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_mm_parallel(data, n_jobs):\n",
    "    import numpy as np\n",
    "    from IPython.lib.backgroundjobs import BackgroundJobManager\n",
    "    \n",
    "    jobs = BackgroundJobManager()\n",
    "    \n",
    "    # each job will get df chunk, prepare mm input and give it to mm, returning path to mm output file\n",
    "    def mm_job(i, _df):\n",
    "        mm_in = prepare_mm_input(_df, os.path.join(data_dir, 'mm_in_{}.txt'.format(i)), verbose=False)\n",
    "        m_out = run_metamap(mm_dir, mm_in, os.path.join(data_dir, 'mm_out_{}.txt'.format(i)), verbose=False)\n",
    "        return m_out\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    print 'Splitting dataframe..'\n",
    "    dfs = np.array_split(data, n_jobs)\n",
    "    \n",
    "    print 'Done. Starting jobs..'\n",
    "    for i, _df in enumerate(dfs):\n",
    "        jobs.new(mm_job, i, _df, daemon=True)\n",
    "    \n",
    "    # Ping jobs status each 10 seconds while we dont complete all jobs or have error.\n",
    "    while len(jobs.dead) == 0 and len(jobs.completed) < n_jobs:\n",
    "        time.sleep(10)\n",
    "        print '%d/%d jobs completed..' % (len(jobs.completed), n_jobs)\n",
    "\n",
    "    print '%d/%d jobs completed. Jobs results:' % (len(jobs.completed), n_jobs)\n",
    "    for job_n in jobs.all.keys():\n",
    "        print jobs.result(job_n)\n",
    "\n",
    "    jobs.flush()\n",
    "    print 'Done. Parallel processing time: %.1f sec' % round(time.time() - start, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_mm(data, conf):\n",
    "    \n",
    "    # split data if we have data_limit in config\n",
    "    _data = data if not conf['data_limit'] else data[:conf['data_limit']]\n",
    "    \n",
    "    if conf['parallel']:\n",
    "        print 'Processing %d records in parallel with %d jobs..' % (_data.shape[0], conf['n_jobs'])\n",
    "        process_mm_parallel(_data, conf['n_jobs'])\n",
    "    else:\n",
    "        print 'Processing %d records in serial..' % _data.shape[0]\n",
    "        process_mm_serial(_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings\n",
    "Here you can configure metamap processing options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MM_CONFIG = {\n",
    "    'parallel': True, # parallel execution gives X2 speed up\n",
    "    'n_jobs': 2, # process all the data in ~1.5 hour in my setup\n",
    "    'data_limit': None # set it to None to process all data  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = __mkdir(working_dir, \"data\")\n",
    "\n",
    "criteria = pd.read_pickle(os.path.join(data_dir, \"ct_filtered.pckl\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>NctId</th>\n",
       "      <th>Ngrams</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>criteria_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(History, history), (of, of), (uncontrolled, ...</td>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>[((history, NN),), ((seizure, NNS),), ((presen...</td>\n",
       "      <td>[(History, NN), (of, IN), (uncontrolled, VBN),...</td>\n",
       "      <td>[History, of, uncontrolled, seizures, at, the,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(Seizure, seizure), (frequency, frequency), (...</td>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>[((seizure, NN),), ((frequency, NN),), ((histo...</td>\n",
       "      <td>[(Seizure, NN), (frequency, NN), (by, IN), (hi...</td>\n",
       "      <td>[Seizure, frequency, by, history, must, be, su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Lemmas        NctId  \\\n",
       "0  [(History, history), (of, of), (uncontrolled, ...  NCT00001149   \n",
       "1  [(Seizure, seizure), (frequency, frequency), (...  NCT00001149   \n",
       "\n",
       "                                              Ngrams  \\\n",
       "0  [((history, NN),), ((seizure, NNS),), ((presen...   \n",
       "1  [((seizure, NN),), ((frequency, NN),), ((histo...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0  [(History, NN), (of, IN), (uncontrolled, VBN),...   \n",
       "1  [(Seizure, NN), (frequency, NN), (by, IN), (hi...   \n",
       "\n",
       "                                              Tokens  criteria_id  \n",
       "0  [History, of, uncontrolled, seizures, at, the,...            0  \n",
       "1  [Seizure, frequency, by, history, must, be, su...            1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 17144 records in parallel with 2 jobs..\n",
      "Splitting dataframe..\n",
      "Done. Starting jobs..\n",
      "Starting job # 0 in a separate thread.\n",
      "Starting job # 2 in a separate thread.\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "0/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "1/2 jobs completed..\n",
      "2/2 jobs completed..\n",
      "2/2 jobs completed. Jobs results:\n",
      "C:\\Study\\CS102\\project\\project2\\repro\\CS109Project\\data\\mm_out_0.txt\n",
      "C:\\Study\\CS102\\project\\project2\\repro\\CS109Project\\data\\mm_out_1.txt\n",
      "Flushing 2 Completed jobs.\n",
      "Done. Parallel processing time: 4800.5 sec\n"
     ]
    }
   ],
   "source": [
    "process_mm(criteria, MM_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process MetaMap results\n",
    "\n",
    "Convert the MetaMap-processed data to a Pandas dataframe and use the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pass_threshold(score):\n",
    "    return score >= 5.5\n",
    "\n",
    "def metamap_to_dataframe(src_file):\n",
    "    print(\"Converting MetaMap results in %s to a dataframe.\" % src_file)\n",
    "    data = []\n",
    "    with open(src_file, 'r') as fh:\n",
    "        for line in fh:\n",
    "            row = line.split(\"|\")\n",
    "            id = row[0]\n",
    "            cri_id, ngrami = map(int, id.split(\"-\"))\n",
    "            score = float(row[2])\n",
    "            term = row[3]\n",
    "            cui = row[4]\n",
    "            stype = row[5].strip(\"[]\")\n",
    "            cid = row[-1].strip().split(\";\")\n",
    "            if pass_threshold(score):\n",
    "                data.append([cri_id,ngrami,score,term,cui,stype,cid])\n",
    "    df = pd.DataFrame(columns=[\"criteria_id\", \"ngram_index\", \"score\", \"term\", \"cui\", \"stype\", \"cid\"], data=data)\n",
    "    return df\n",
    "\n",
    "def process_mm_results(conf):\n",
    "    if conf['parallel']:\n",
    "        # collect mm_out_... files, transform them to dataframes & concat in resulting mm:\n",
    "        mm_dfs = []\n",
    "        for i in range(conf['n_jobs']):\n",
    "            mm_df = metamap_to_dataframe(os.path.join(data_dir, \"mm_out_{}.txt\".format(i)))\n",
    "            mm_dfs.append(mm_df)\n",
    "        mm = pd.concat(mm_dfs)\n",
    "        return mm\n",
    "    else:\n",
    "        mm = metamap_to_dataframe(os.path.join(data_dir, \"mm_out.txt\"))\n",
    "        return mm\n",
    "    \n",
    "def extend_with_nctids(mm_df, criteria_df):\n",
    "    nctids = []\n",
    "    for row in mm_df[['criteria_id']].itertuples():\n",
    "        cri_id = row[1]\n",
    "        nct_id = criteria_df[criteria_df['criteria_id'] == cri_id].NctId.values[0]\n",
    "        nctids.append(nct_id)\n",
    "    mm_df.insert(0, 'nct_id', nctids)\n",
    "    return mm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting MetaMap results in C:\\Study\\CS102\\project\\project2\\repro\\CS109Project\\data\\mm_out_0.txt to a dataframe.\n",
      "Converting MetaMap results in C:\\Study\\CS102\\project\\project2\\repro\\CS109Project\\data\\mm_out_1.txt to a dataframe.\n"
     ]
    }
   ],
   "source": [
    "data_dir = __mkdir(working_dir, \"data\")\n",
    "\n",
    "mm = process_mm_results(MM_CONFIG)\n",
    "\n",
    "# we can get NctId by criteria id from criterias df:\n",
    "criteria = pd.read_pickle(os.path.join(data_dir, 'ct_criteria.pckl'))\n",
    "mm = extend_with_nctids(mm, criteria)\n",
    "\n",
    "mm.to_pickle(os.path.join(data_dir, \"mm.pckl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the serialized MetaMap results and display sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_id</th>\n",
       "      <th>criteria_id</th>\n",
       "      <th>ngram_index</th>\n",
       "      <th>score</th>\n",
       "      <th>term</th>\n",
       "      <th>cui</th>\n",
       "      <th>stype</th>\n",
       "      <th>cid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.49</td>\n",
       "      <td>Historical aspects qualifier</td>\n",
       "      <td>C0019665</td>\n",
       "      <td>inpr</td>\n",
       "      <td>[x.x.x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.34</td>\n",
       "      <td>History</td>\n",
       "      <td>C0019664</td>\n",
       "      <td>ocdi</td>\n",
       "      <td>[K01.400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.80</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8.34</td>\n",
       "      <td>Time</td>\n",
       "      <td>C0040223</td>\n",
       "      <td>tmco</td>\n",
       "      <td>[G01.910]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17.80</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>9.95</td>\n",
       "      <td>Historical aspects qualifier</td>\n",
       "      <td>C0019665</td>\n",
       "      <td>inpr</td>\n",
       "      <td>[x.x.x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>6.79</td>\n",
       "      <td>History</td>\n",
       "      <td>C0019664</td>\n",
       "      <td>ocdi</td>\n",
       "      <td>[K01.400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>16.26</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>16.26</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6.79</td>\n",
       "      <td>Time</td>\n",
       "      <td>C0040223</td>\n",
       "      <td>tmco</td>\n",
       "      <td>[G01.910]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>6.79</td>\n",
       "      <td>Time</td>\n",
       "      <td>C0040223</td>\n",
       "      <td>tmco</td>\n",
       "      <td>[G01.910]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>16.26</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>16.26</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>9.90</td>\n",
       "      <td>Historical aspects qualifier</td>\n",
       "      <td>C0019665</td>\n",
       "      <td>inpr</td>\n",
       "      <td>[x.x.x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>6.74</td>\n",
       "      <td>History</td>\n",
       "      <td>C0019664</td>\n",
       "      <td>ocdi</td>\n",
       "      <td>[K01.400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>16.21</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>16.21</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>16.21</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>6.74</td>\n",
       "      <td>Time</td>\n",
       "      <td>C0040223</td>\n",
       "      <td>tmco</td>\n",
       "      <td>[G01.910]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>6.74</td>\n",
       "      <td>Time</td>\n",
       "      <td>C0040223</td>\n",
       "      <td>tmco</td>\n",
       "      <td>[G01.910]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>6.74</td>\n",
       "      <td>Time</td>\n",
       "      <td>C0040223</td>\n",
       "      <td>tmco</td>\n",
       "      <td>[G01.910]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>16.07</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>16.21</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>16.21</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11.49</td>\n",
       "      <td>Historical aspects qualifier</td>\n",
       "      <td>C0019665</td>\n",
       "      <td>inpr</td>\n",
       "      <td>[x.x.x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8.34</td>\n",
       "      <td>History</td>\n",
       "      <td>C0019664</td>\n",
       "      <td>ocdi</td>\n",
       "      <td>[K01.400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8.34</td>\n",
       "      <td>Patients</td>\n",
       "      <td>C0030705</td>\n",
       "      <td>podg</td>\n",
       "      <td>[M01.643]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>17.80</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>C0036572</td>\n",
       "      <td>sosy</td>\n",
       "      <td>[C10.228.140.490.631, C10.597.742, C23.888.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NCT00001149</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>12.94</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>C0006539</td>\n",
       "      <td>geoa</td>\n",
       "      <td>[Z01.542.248.700, Z01.586.200.650, Z01.586.950...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>9.95</td>\n",
       "      <td>Historical aspects qualifier</td>\n",
       "      <td>C0019665</td>\n",
       "      <td>inpr</td>\n",
       "      <td>[x.x.x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>6.79</td>\n",
       "      <td>History</td>\n",
       "      <td>C0019664</td>\n",
       "      <td>ocdi</td>\n",
       "      <td>[K01.400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>17.80</td>\n",
       "      <td>Epilepsies, Partial</td>\n",
       "      <td>C0014547</td>\n",
       "      <td>dsyn</td>\n",
       "      <td>[C10.228.140.490.360]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>13.10</td>\n",
       "      <td>Epilepsy</td>\n",
       "      <td>C0014544</td>\n",
       "      <td>dsyn</td>\n",
       "      <td>[C10.228.140.490]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>51</td>\n",
       "      <td>11</td>\n",
       "      <td>9.90</td>\n",
       "      <td>Historical aspects qualifier</td>\n",
       "      <td>C0019665</td>\n",
       "      <td>inpr</td>\n",
       "      <td>[x.x.x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>51</td>\n",
       "      <td>11</td>\n",
       "      <td>6.74</td>\n",
       "      <td>History</td>\n",
       "      <td>C0019664</td>\n",
       "      <td>ocdi</td>\n",
       "      <td>[K01.400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "      <td>20.95</td>\n",
       "      <td>Epilepsy, Complex Partial</td>\n",
       "      <td>C0085417</td>\n",
       "      <td>dsyn</td>\n",
       "      <td>[C10.228.140.490.360.260]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>51</td>\n",
       "      <td>15</td>\n",
       "      <td>16.33</td>\n",
       "      <td>Epilepsies, Partial</td>\n",
       "      <td>C0014547</td>\n",
       "      <td>dsyn</td>\n",
       "      <td>[C10.228.140.490.360]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>13.05</td>\n",
       "      <td>Epilepsy</td>\n",
       "      <td>C0014544</td>\n",
       "      <td>dsyn</td>\n",
       "      <td>[C10.228.140.490]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>8.34</td>\n",
       "      <td>Patients</td>\n",
       "      <td>C0030705</td>\n",
       "      <td>podg</td>\n",
       "      <td>[M01.643]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>17.80</td>\n",
       "      <td>Treatment Protocols</td>\n",
       "      <td>C0040808</td>\n",
       "      <td>topp</td>\n",
       "      <td>[E02.183, N05.715.360.775.225]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>8.34</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>C0025118</td>\n",
       "      <td>bmod</td>\n",
       "      <td>[H02.403]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>17.80</td>\n",
       "      <td>In Blood</td>\n",
       "      <td>C0005768</td>\n",
       "      <td>bdsu</td>\n",
       "      <td>[x.x.x.x.x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>11.49</td>\n",
       "      <td>Blood</td>\n",
       "      <td>C0005767</td>\n",
       "      <td>tisu</td>\n",
       "      <td>[A12.207.152, A15.145]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>19.41</td>\n",
       "      <td>Convulsants</td>\n",
       "      <td>C0009950</td>\n",
       "      <td>hops,phsu</td>\n",
       "      <td>[D27.505.696.282.224, D27.505.954.427.220.224]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>6.79</td>\n",
       "      <td>Patients</td>\n",
       "      <td>C0030705</td>\n",
       "      <td>podg</td>\n",
       "      <td>[M01.643]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>19.36</td>\n",
       "      <td>Convulsants</td>\n",
       "      <td>C0009950</td>\n",
       "      <td>hops,phsu</td>\n",
       "      <td>[D27.505.696.282.224, D27.505.954.427.220.224]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>19</td>\n",
       "      <td>19.22</td>\n",
       "      <td>Convulsants</td>\n",
       "      <td>C0009950</td>\n",
       "      <td>hops,phsu</td>\n",
       "      <td>[D27.505.696.282.224, D27.505.954.427.220.224]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>19</td>\n",
       "      <td>16.21</td>\n",
       "      <td>Treatment Protocols</td>\n",
       "      <td>C0040808</td>\n",
       "      <td>topp</td>\n",
       "      <td>[E02.183, N05.715.360.775.225]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>16.26</td>\n",
       "      <td>Treatment Protocols</td>\n",
       "      <td>C0040808</td>\n",
       "      <td>topp</td>\n",
       "      <td>[E02.183, N05.715.360.775.225]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>21</td>\n",
       "      <td>6.79</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>C0025118</td>\n",
       "      <td>bmod</td>\n",
       "      <td>[H02.403]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>22</td>\n",
       "      <td>6.79</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>C0025118</td>\n",
       "      <td>bmod</td>\n",
       "      <td>[H02.403]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>26</td>\n",
       "      <td>16.26</td>\n",
       "      <td>In Blood</td>\n",
       "      <td>C0005768</td>\n",
       "      <td>bdsu</td>\n",
       "      <td>[x.x.x.x.x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>26</td>\n",
       "      <td>9.95</td>\n",
       "      <td>Blood</td>\n",
       "      <td>C0005767</td>\n",
       "      <td>tisu</td>\n",
       "      <td>[A12.207.152, A15.145]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>27</td>\n",
       "      <td>16.26</td>\n",
       "      <td>In Blood</td>\n",
       "      <td>C0005768</td>\n",
       "      <td>bdsu</td>\n",
       "      <td>[x.x.x.x.x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>27</td>\n",
       "      <td>9.95</td>\n",
       "      <td>Blood</td>\n",
       "      <td>C0005767</td>\n",
       "      <td>tisu</td>\n",
       "      <td>[A12.207.152, A15.145]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>29</td>\n",
       "      <td>19.36</td>\n",
       "      <td>Convulsants</td>\n",
       "      <td>C0009950</td>\n",
       "      <td>hops,phsu</td>\n",
       "      <td>[D27.505.696.282.224, D27.505.954.427.220.224]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>19.36</td>\n",
       "      <td>Convulsants</td>\n",
       "      <td>C0009950</td>\n",
       "      <td>hops,phsu</td>\n",
       "      <td>[D27.505.696.282.224, D27.505.954.427.220.224]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>9.76</td>\n",
       "      <td>Volition</td>\n",
       "      <td>C0042950</td>\n",
       "      <td>menp</td>\n",
       "      <td>[F02.463.902]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>NCT00001666</td>\n",
       "      <td>52</td>\n",
       "      <td>44</td>\n",
       "      <td>9.79</td>\n",
       "      <td>Volition</td>\n",
       "      <td>C0042950</td>\n",
       "      <td>menp</td>\n",
       "      <td>[F02.463.902]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          nct_id  criteria_id  ngram_index  score  \\\n",
       "0    NCT00001149            0            0  11.49   \n",
       "1    NCT00001149            0            0   8.34   \n",
       "2    NCT00001149            0            1  17.80   \n",
       "3    NCT00001149            0            3   8.34   \n",
       "4    NCT00001149            0            7  17.80   \n",
       "5    NCT00001149            0           10   9.95   \n",
       "6    NCT00001149            0           10   6.79   \n",
       "7    NCT00001149            0           11  16.26   \n",
       "8    NCT00001149            0           12  16.26   \n",
       "9    NCT00001149            0           14   6.79   \n",
       "10   NCT00001149            0           15   6.79   \n",
       "11   NCT00001149            0           22  16.26   \n",
       "12   NCT00001149            0           23  16.26   \n",
       "13   NCT00001149            0           27   9.90   \n",
       "14   NCT00001149            0           27   6.74   \n",
       "15   NCT00001149            0           28  16.21   \n",
       "16   NCT00001149            0           29  16.21   \n",
       "17   NCT00001149            0           30  16.21   \n",
       "18   NCT00001149            0           32   6.74   \n",
       "19   NCT00001149            0           33   6.74   \n",
       "20   NCT00001149            0           34   6.74   \n",
       "21   NCT00001149            0           43  16.07   \n",
       "22   NCT00001149            0           44  16.21   \n",
       "23   NCT00001149            0           45  16.21   \n",
       "24   NCT00001149            1            0  17.80   \n",
       "25   NCT00001149            1            2  11.49   \n",
       "26   NCT00001149            1            2   8.34   \n",
       "27   NCT00001149            1            5   8.34   \n",
       "28   NCT00001149            1            6  17.80   \n",
       "29   NCT00001149            1           10  12.94   \n",
       "..           ...          ...          ...    ...   \n",
       "970  NCT00001666           51            5   9.95   \n",
       "971  NCT00001666           51            5   6.79   \n",
       "972  NCT00001666           51            8  17.80   \n",
       "973  NCT00001666           51            9  13.10   \n",
       "974  NCT00001666           51           11   9.90   \n",
       "975  NCT00001666           51           11   6.74   \n",
       "976  NCT00001666           51           14  20.95   \n",
       "977  NCT00001666           51           15  16.33   \n",
       "978  NCT00001666           51           16  13.05   \n",
       "979  NCT00001666           52            0   8.34   \n",
       "980  NCT00001666           52            2  17.80   \n",
       "981  NCT00001666           52            3   8.34   \n",
       "982  NCT00001666           52            6  17.80   \n",
       "983  NCT00001666           52            6  11.49   \n",
       "984  NCT00001666           52            8  19.41   \n",
       "985  NCT00001666           52           16   6.79   \n",
       "986  NCT00001666           52           18  19.36   \n",
       "987  NCT00001666           52           19  19.22   \n",
       "988  NCT00001666           52           19  16.21   \n",
       "989  NCT00001666           52           20  16.26   \n",
       "990  NCT00001666           52           21   6.79   \n",
       "991  NCT00001666           52           22   6.79   \n",
       "992  NCT00001666           52           26  16.26   \n",
       "993  NCT00001666           52           26   9.95   \n",
       "994  NCT00001666           52           27  16.26   \n",
       "995  NCT00001666           52           27   9.95   \n",
       "996  NCT00001666           52           29  19.36   \n",
       "997  NCT00001666           52           30  19.36   \n",
       "998  NCT00001666           52           30   9.76   \n",
       "999  NCT00001666           52           44   9.79   \n",
       "\n",
       "                             term       cui      stype  \\\n",
       "0    Historical aspects qualifier  C0019665       inpr   \n",
       "1                         History  C0019664       ocdi   \n",
       "2                        Seizures  C0036572       sosy   \n",
       "3                            Time  C0040223       tmco   \n",
       "4                        Seizures  C0036572       sosy   \n",
       "5    Historical aspects qualifier  C0019665       inpr   \n",
       "6                         History  C0019664       ocdi   \n",
       "7                        Seizures  C0036572       sosy   \n",
       "8                        Seizures  C0036572       sosy   \n",
       "9                            Time  C0040223       tmco   \n",
       "10                           Time  C0040223       tmco   \n",
       "11                       Seizures  C0036572       sosy   \n",
       "12                       Seizures  C0036572       sosy   \n",
       "13   Historical aspects qualifier  C0019665       inpr   \n",
       "14                        History  C0019664       ocdi   \n",
       "15                       Seizures  C0036572       sosy   \n",
       "16                       Seizures  C0036572       sosy   \n",
       "17                       Seizures  C0036572       sosy   \n",
       "18                           Time  C0040223       tmco   \n",
       "19                           Time  C0040223       tmco   \n",
       "20                           Time  C0040223       tmco   \n",
       "21                       Seizures  C0036572       sosy   \n",
       "22                       Seizures  C0036572       sosy   \n",
       "23                       Seizures  C0036572       sosy   \n",
       "24                       Seizures  C0036572       sosy   \n",
       "25   Historical aspects qualifier  C0019665       inpr   \n",
       "26                        History  C0019664       ocdi   \n",
       "27                       Patients  C0030705       podg   \n",
       "28                       Seizures  C0036572       sosy   \n",
       "29                        Belarus  C0006539       geoa   \n",
       "..                            ...       ...        ...   \n",
       "970  Historical aspects qualifier  C0019665       inpr   \n",
       "971                       History  C0019664       ocdi   \n",
       "972           Epilepsies, Partial  C0014547       dsyn   \n",
       "973                      Epilepsy  C0014544       dsyn   \n",
       "974  Historical aspects qualifier  C0019665       inpr   \n",
       "975                       History  C0019664       ocdi   \n",
       "976     Epilepsy, Complex Partial  C0085417       dsyn   \n",
       "977           Epilepsies, Partial  C0014547       dsyn   \n",
       "978                      Epilepsy  C0014544       dsyn   \n",
       "979                      Patients  C0030705       podg   \n",
       "980           Treatment Protocols  C0040808       topp   \n",
       "981                      Medicine  C0025118       bmod   \n",
       "982                      In Blood  C0005768       bdsu   \n",
       "983                         Blood  C0005767       tisu   \n",
       "984                   Convulsants  C0009950  hops,phsu   \n",
       "985                      Patients  C0030705       podg   \n",
       "986                   Convulsants  C0009950  hops,phsu   \n",
       "987                   Convulsants  C0009950  hops,phsu   \n",
       "988           Treatment Protocols  C0040808       topp   \n",
       "989           Treatment Protocols  C0040808       topp   \n",
       "990                      Medicine  C0025118       bmod   \n",
       "991                      Medicine  C0025118       bmod   \n",
       "992                      In Blood  C0005768       bdsu   \n",
       "993                         Blood  C0005767       tisu   \n",
       "994                      In Blood  C0005768       bdsu   \n",
       "995                         Blood  C0005767       tisu   \n",
       "996                   Convulsants  C0009950  hops,phsu   \n",
       "997                   Convulsants  C0009950  hops,phsu   \n",
       "998                      Volition  C0042950       menp   \n",
       "999                      Volition  C0042950       menp   \n",
       "\n",
       "                                                   cid  \n",
       "0                                              [x.x.x]  \n",
       "1                                            [K01.400]  \n",
       "2    [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "3                                            [G01.910]  \n",
       "4    [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "5                                              [x.x.x]  \n",
       "6                                            [K01.400]  \n",
       "7    [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "8    [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "9                                            [G01.910]  \n",
       "10                                           [G01.910]  \n",
       "11   [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "12   [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "13                                             [x.x.x]  \n",
       "14                                           [K01.400]  \n",
       "15   [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "16   [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "17   [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "18                                           [G01.910]  \n",
       "19                                           [G01.910]  \n",
       "20                                           [G01.910]  \n",
       "21   [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "22   [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "23   [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "24   [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "25                                             [x.x.x]  \n",
       "26                                           [K01.400]  \n",
       "27                                           [M01.643]  \n",
       "28   [C10.228.140.490.631, C10.597.742, C23.888.592...  \n",
       "29   [Z01.542.248.700, Z01.586.200.650, Z01.586.950...  \n",
       "..                                                 ...  \n",
       "970                                            [x.x.x]  \n",
       "971                                          [K01.400]  \n",
       "972                              [C10.228.140.490.360]  \n",
       "973                                  [C10.228.140.490]  \n",
       "974                                            [x.x.x]  \n",
       "975                                          [K01.400]  \n",
       "976                          [C10.228.140.490.360.260]  \n",
       "977                              [C10.228.140.490.360]  \n",
       "978                                  [C10.228.140.490]  \n",
       "979                                          [M01.643]  \n",
       "980                     [E02.183, N05.715.360.775.225]  \n",
       "981                                          [H02.403]  \n",
       "982                                        [x.x.x.x.x]  \n",
       "983                             [A12.207.152, A15.145]  \n",
       "984     [D27.505.696.282.224, D27.505.954.427.220.224]  \n",
       "985                                          [M01.643]  \n",
       "986     [D27.505.696.282.224, D27.505.954.427.220.224]  \n",
       "987     [D27.505.696.282.224, D27.505.954.427.220.224]  \n",
       "988                     [E02.183, N05.715.360.775.225]  \n",
       "989                     [E02.183, N05.715.360.775.225]  \n",
       "990                                          [H02.403]  \n",
       "991                                          [H02.403]  \n",
       "992                                        [x.x.x.x.x]  \n",
       "993                             [A12.207.152, A15.145]  \n",
       "994                                        [x.x.x.x.x]  \n",
       "995                             [A12.207.152, A15.145]  \n",
       "996     [D27.505.696.282.224, D27.505.954.427.220.224]  \n",
       "997     [D27.505.696.282.224, D27.505.954.427.220.224]  \n",
       "998                                      [F02.463.902]  \n",
       "999                                      [F02.463.902]  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = __mkdir(working_dir, \"data\")\n",
    "mm = pd.read_pickle(os.path.join(data_dir, \"mm.pckl\"))\n",
    "mm.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
