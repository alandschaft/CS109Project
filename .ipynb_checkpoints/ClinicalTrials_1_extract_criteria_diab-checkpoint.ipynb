{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup\n",
    "\n",
    "Let's import the required libraries and set up global variables for the rest of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): tqdm in /anaconda/lib/python2.7/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "!pip install tqdm\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import zipfile\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from lxml import objectify\n",
    "import codecs\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tarfile\n",
    "import subprocess\n",
    "import platform\n",
    "import time\n",
    "from tqdm import tqdm as progressbar # pandas df usage: 'for row in progressbar(df.itertuples(), total=df.shape[0])'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to create a directory under the specified path, gracefully handling errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __mkdir(*args):\n",
    "    path = os.path.join(*args)\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/Lo/Work/CS109Project\n",
      "Download directory: /Users/Lo/Work/CS109Project/download_diab\n"
     ]
    }
   ],
   "source": [
    "# Create the project directory holding the downloaded data, serialized dataframes and MetaMap install.\n",
    "# working_dir = __mkdir(os.path.expanduser(\"~\"), \"Medframes\")\n",
    "\n",
    "# Set working directory as the current directory of the ipython notebook\n",
    "working_dir = os.getcwd()\n",
    "download_dir = __mkdir(working_dir, \"download_diab\")\n",
    "print(\"Working directory: %s\" % working_dir)\n",
    "print(\"Download directory: %s\" % download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download\n",
    "\n",
    "Download CSV data from clinicaltrials.gov. The data will be written in the working directory specified above as  [data_dir]/study_fields.csv.\n",
    "\n",
    "For clinicaltrials.gov, a search term needs to be specified. In this example, we'll download search results for the term \"seizure\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_ctgov(dest_dir, search_term):\n",
    "    print(\"Downloading clinicaltrials.gov results for '%s' to %s\" % (search_term, dest_dir))\n",
    "    dl_url = \"https://clinicaltrials.gov/ct2/results/download?down_stds=all&down_typ=results&down_flds=all&down_fmt=xml&term=%s&show_down=Y\" % search_term\n",
    "\n",
    "    # Download the zipped data and extract it to the output directory\n",
    "    out_path = os.path.join(dest_dir, \"download_ctgov.zip\")\n",
    "    with open(out_path, 'wb') as fh:\n",
    "        r = requests.get(dl_url)\n",
    "        for block in r.iter_content(1024):\n",
    "            fh.write(block)\n",
    "    with zipfile.ZipFile(out_path, 'r') as z:\n",
    "        z.extractall(dest_dir)\n",
    "    return dest_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_ctgov(download_dir, \"type 2 diabetes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas import\n",
    "\n",
    "Convert the downloaded CSV data to Pandas dataframes and serialize them as Python pickles. The function reads XML files from the working directory and writes to \"ctgov.pckl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ctgov_to_dataframe(src_dir):\n",
    "    # Get all XML files in the data directory\n",
    "    print(\"Transforming cliniclatrials download (%s) to dataframe\" % (src_dir))\n",
    "    data = []\n",
    "    for f in [_ for _ in os.listdir(src_dir) if _.endswith('.xml')]:\n",
    "        xml = objectify.parse(os.path.join(src_dir, f))\n",
    "        root = xml.getroot()\n",
    "        d = defaultdict(list)\n",
    "        for t in root.iter():\n",
    "            if t.text:\n",
    "                key = re.sub(r'\\[\\d+\\]', '', xml.getpath(t)).replace('/clinical_study/', '').replace('/', '.')\n",
    "                val = t.text.strip()\n",
    "                d[key].append(val)\n",
    "        d = {k: v[0] if len(v) == 1 else v for k, v in d.items()}\n",
    "        data.append(d)\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing dataframes\n",
    "Transform the downloaded data to Pandas dataframes and seialize them as Python pickles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_dir = __mkdir(working_dir, \"download_diab\")\n",
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "ct_df = ctgov_to_dataframe(download_dir)\n",
    "ct_df.to_pickle(os.path.join(data_dir, 'ctgov.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataframes\n",
    "\n",
    "Read the pickled data back into Pandas and display the first 5 records. In this example, the pickled dataframe is serialized to \"ctgov.pckl\" in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "ctgov_data = pd.read_pickle(os.path.join(data_dir, 'ctgov.pckl'))\n",
    "ctgov_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract criteria\n",
    "\n",
    "Read in the serialized data from clinicaltrials.gov and extract inclusion/exlcusion criteria, one per row. Output a Series(id_info.nct_id, Criteria, Inclusion, TokenCount)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def id_generator(first_val=0, inc_func=lambda val: val + 1):\n",
    "    \"\"\"\n",
    "        Simple id generator. It takes first val & increase function and yields ids as needed.\n",
    "        Will return integers starting from 0 by default.\n",
    "    \"\"\"\n",
    "    id = first_val\n",
    "    while True:\n",
    "        yield id\n",
    "        id = inc_func(id)\n",
    "        \n",
    "        \n",
    "def __process_criteria(data, get_criteria_id):\n",
    "    \"\"\"\n",
    "        Extract inclusion and exclusion criteria from the clinical trials data.\n",
    "        Then - Tokenize and write the extracted data to a frame.\n",
    "    \"\"\"\n",
    "    pat = r\"^([\\w\\-]*\\s*){0,5}%s criteria[\\s\\w\\(\\),]*:\"\n",
    "    inpat = re.compile(pat % 'inclusion', re.UNICODE)\n",
    "    expat = re.compile(pat % 'exclusion', re.UNICODE)\n",
    "    try:\n",
    "        incl = True\n",
    "        nct_id = data[1]\n",
    "        txt = [_.strip() for _ in data[2].split(u'\\n\\n')]\n",
    "        for l in txt:\n",
    "            if re.match(inpat, l.lower()):\n",
    "                incl = True\n",
    "            elif re.match(expat, l.lower()):\n",
    "                incl = False\n",
    "            else:\n",
    "                toks = nltk.word_tokenize(l)\n",
    "                cri_id = next(get_criteria_id)\n",
    "                s = {'criteria_id': cri_id, 'NctId': nct_id, 'Criteria': unicode(l), 'Include': incl, 'Tokens': toks, 'TokenCount': len(toks)}\n",
    "                yield s\n",
    "    except Exception as e:\n",
    "        print(\"Error processing row %s: %s\" % (data[2], e))\n",
    "\n",
    "        \n",
    "def extract_criteria(data):\n",
    "    \"\"\"\n",
    "        Extract inclusion and exclusion criteria from each clinical trial into\n",
    "    \"\"\"\n",
    "    print(\"Transforming data (extracting criteria)\")\n",
    "    criteria_id_generator = id_generator()\n",
    "    transformed = [s for row in data[['id_info.nct_id', 'eligibility.criteria.textblock']].itertuples() for s in\n",
    "                   __process_criteria(row, criteria_id_generator)]\n",
    "    df = pd.DataFrame(transformed)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data and write the result to a file. (You'll notice that the script logs an error for one row. This is expected and results from that row being a \"NaN\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "ctgov_data = pd.read_pickle(os.path.join(data_dir, 'ctgov.pckl'))\n",
    "# Extract criteria\n",
    "criteria = extract_criteria(ctgov_data)\n",
    "criteria.to_pickle(os.path.join(data_dir, 'ct_criteria.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back the data and display a record selected by column value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criteria = pd.read_pickle(os.path.join(data_dir, 'ct_criteria.pckl'))\n",
    "criteria.loc[criteria['criteria_id'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criteria.loc[criteria['NctId'] == 'NCT01373190']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag, lemmatize, ngrammize\n",
    "\n",
    "Processes the extracted criteria with the NLTK POS tagger and lemmatizer and generates ngrams of 1-3 words (note: while unigrams are technically duplicated as 'Tokens', it will be more convenient to allow this and keep them in one column with bigrams and trigrams). Preprocesses the tokens by removing special characters and punctuation. Lemmata and ngrams are lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __lemmatise(lemmatizer, r):\n",
    "    wn_tags = {'NN': nltk.corpus.wordnet.NOUN, 'JJ': nltk.corpus.wordnet.ADJ, 'VB': nltk.corpus.wordnet.VERB,\n",
    "               'RB': nltk.corpus.wordnet.ADV}\n",
    "    return [(t[0], lemmatizer.lemmatize(t[0].lower(), pos=wn_tags.get(t[1][:2], nltk.corpus.wordnet.NOUN)).lower()) for\n",
    "            t in r]\n",
    "\n",
    "\n",
    "def tag_and_stem(data):\n",
    "    print(\"Transforming data (tagging and lemmatising)\")\n",
    "    series = []\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    punct = '[%s]*' % re.escape(string.punctuation)\n",
    "    pat = re.compile(r\"^(%(p)s[\\w\\d]+%(p)s)+$\" % {'p': punct}, re.UNICODE)\n",
    "    # Itertuples is 50% faster than df.apply()\n",
    "    for row in progressbar(data[['NctId', 'Tokens', 'criteria_id']].itertuples(), total=data.shape[0]):\n",
    "        nct_id = row[1]\n",
    "        toks = filter(lambda t: re.match(pat, t), row[2])\n",
    "        cri_id = row[3]\n",
    "        tags = nltk.pos_tag(toks)\n",
    "        lemmas = __lemmatise(lemmatizer, tags)\n",
    "        ngrams = []\n",
    "        for n in (1, 2, 3):\n",
    "            ngrams += list(nltk.ngrams([(lemma[1], tags[idx][1]) for idx, lemma in enumerate(lemmas)], n))\n",
    "        s = {'criteria_id': cri_id, 'NctId': nct_id, 'Tokens': toks, 'Tags': tags, 'Lemmas': lemmas, 'Ngrams': ngrams}\n",
    "        series.append(s)\n",
    "    df = pd.DataFrame(series)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the extracted criteria (stored in \"ct_criteria.pckl\" in the previous step), tag, lemmatize and ngrammize the data and store it as \"ct_tagged.pckl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "\n",
    "criteria = pd.read_pickle(os.path.join(data_dir, 'ct_criteria.pckl'))\n",
    "tagged = tag_and_stem(criteria)\n",
    "tagged.to_pickle(os.path.join(data_dir, 'ct_tagged.pckl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged = pd.read_pickle(os.path.join(data_dir, 'ct_tagged.pckl'))\n",
    "tagged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter criteria\n",
    "Filters out criteria composed entirely of function words and stopwords. Strips ngrams composed entirely of stop words/tags from the ngram list. By default this function uses the NTLK stopword list and all PTB tags except nouns. Additional lists of stop words and stop tags can be supplied with keyword arguments (\"stop_words\", \"stop_tags\"). Returns a tuple of dataframes, (filtered_criteria, excluded_criteria).\n",
    "\n",
    "(Note: this step generates a SettingWithCopyWarning. This is known and is a false positive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __filter(values, idx, stops):\n",
    "    return not set([t[idx] for t in values]) <= stops\n",
    "\n",
    "\n",
    "def filter_criteria(data, user_stop_words=[], user_stop_tags=[]):\n",
    "    print(\"Filtering criteria\")\n",
    "    default_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    default_stop_tags = [\"$\", \"''\", \"(\", \")\", \",\", \"--\", \".\", \":\", \"CC\", \"CD\", \"DT\",\n",
    "                         \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\",\n",
    "                         \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\",\n",
    "                         \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\",\n",
    "                         \"WDT\", \"WP\", \"WP$\", \"WRB\", \"``\"]\n",
    "    print(\"Filtering stops\")\n",
    "    stop_words = set(default_stop_words + user_stop_words)\n",
    "    stop_tags = set(default_stop_tags + user_stop_tags)\n",
    "    excluded = pd.DataFrame()\n",
    "    for col, idx, stops in (\n",
    "            ('Lemmas', 0, stop_words),\n",
    "            ('Tags', 1, stop_tags)):  # Lemma filtering excludes 18 rows, tag filtering excludes 205\n",
    "        data['Ngrams'] = data['Ngrams'].apply(lambda row: [ngram for ngram in row if __filter(ngram, idx, stops)])\n",
    "        groups = data.groupby(lambda r: __filter(data[col].loc[r], 1, stops))\n",
    "        data = groups.get_group(True)\n",
    "        excluded = excluded.append(groups.get_group(False)) if groups.groups.has_key(False) else excluded\n",
    "    return (data, excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the tagged criteria (stored in \"ct_tagged.pckl\" in the previous step), filter out noise and write the results to \"ct_filtered.pckl\" (the included criteria) and \"ct_excluded.pckl\" (the excluded criteria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "\n",
    "criteria = pd.read_pickle(os.path.join(data_dir, 'ct_tagged.pckl'))\n",
    "incl, excl = filter_criteria(criteria)\n",
    "incl.to_pickle(os.path.join(data_dir, 'ct_filtered.pckl'))\n",
    "excl.to_pickle(os.path.join(data_dir, 'ct_excluded.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove ngram duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "incl = pd.read_pickle(os.path.join(data_dir, 'ct_filtered.pckl'))\n",
    "incl['Ngrams'] = incl['Ngrams'].apply(lambda ngrams: list(set(ngrams)))\n",
    "incl.to_pickle(os.path.join(data_dir, 'ct_filtered.pckl'))\n",
    "incl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excl = pd.read_pickle(os.path.join(data_dir, 'ct_excluded.pckl'))\n",
    "excl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
