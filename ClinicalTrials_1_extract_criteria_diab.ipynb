{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup\n",
    "\n",
    "Let's import the required libraries and set up global variables for the rest of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "!pip install tqdm\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import zipfile\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from lxml import objectify\n",
    "import codecs\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tarfile\n",
    "import subprocess\n",
    "import platform\n",
    "import time\n",
    "from tqdm import tqdm as progressbar # pandas df usage: 'for row in progressbar(df.itertuples(), total=df.shape[0])'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to create a directory under the specified path, gracefully handling errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __mkdir(*args):\n",
    "    path = os.path.join(*args)\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the project directory holding the downloaded data, serialized dataframes and MetaMap install.\n",
    "# working_dir = __mkdir(os.path.expanduser(\"~\"), \"Medframes\")\n",
    "\n",
    "# Set working directory as the current directory of the ipython notebook\n",
    "working_dir = os.getcwd()\n",
    "download_dir = __mkdir(working_dir, \"download_diab\")\n",
    "print(\"Working directory: %s\" % working_dir)\n",
    "print(\"Download directory: %s\" % download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download\n",
    "\n",
    "Download CSV data from clinicaltrials.gov. The data will be written in the working directory specified above as  [data_dir]/study_fields.csv.\n",
    "\n",
    "For clinicaltrials.gov, a search term needs to be specified. In this example, we'll download search results for the term \"seizure\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_ctgov(dest_dir, search_term):\n",
    "    print(\"Downloading clinicaltrials.gov results for '%s' to %s\" % (search_term, dest_dir))\n",
    "    dl_url = \"https://clinicaltrials.gov/ct2/results/download?down_stds=all&down_typ=results&down_flds=all&down_fmt=xml&term=%s&show_down=Y\" % search_term\n",
    "\n",
    "    # Download the zipped data and extract it to the output directory\n",
    "    out_path = os.path.join(dest_dir, \"download_ctgov.zip\")\n",
    "    with open(out_path, 'wb') as fh:\n",
    "        r = requests.get(dl_url)\n",
    "        for block in r.iter_content(1024):\n",
    "            fh.write(block)\n",
    "    with zipfile.ZipFile(out_path, 'r') as z:\n",
    "        z.extractall(dest_dir)\n",
    "    return dest_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_ctgov(download_dir, \"type 2 diabetes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas import\n",
    "\n",
    "Convert the downloaded CSV data to Pandas dataframes and serialize them as Python pickles. The function reads XML files from the working directory and writes to \"ctgov.pckl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ctgov_to_dataframe(src_dir):\n",
    "    # Get all XML files in the data directory\n",
    "    print(\"Transforming cliniclatrials download (%s) to dataframe\" % (src_dir))\n",
    "    data = []\n",
    "    for f in [_ for _ in os.listdir(src_dir) if _.endswith('.xml')]:\n",
    "        xml = objectify.parse(os.path.join(src_dir, f))\n",
    "        root = xml.getroot()\n",
    "        d = defaultdict(list)\n",
    "        for t in root.iter():\n",
    "            if t.text:\n",
    "                key = re.sub(r'\\[\\d+\\]', '', xml.getpath(t)).replace('/clinical_study/', '').replace('/', '.')\n",
    "                val = t.text.strip()\n",
    "                d[key].append(val)\n",
    "        d = {k: v[0] if len(v) == 1 else v for k, v in d.items()}\n",
    "        data.append(d)\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing dataframes\n",
    "Transform the downloaded data to Pandas dataframes and seialize them as Python pickles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming cliniclatrials download (/Users/Lo/Work/CS109Project/download_diab) to dataframe\n"
     ]
    }
   ],
   "source": [
    "download_dir = __mkdir(working_dir, \"download_diab\")\n",
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "ct_df = ctgov_to_dataframe(download_dir)\n",
    "ct_df.to_pickle(os.path.join(data_dir, 'ctgov.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataframes\n",
    "\n",
    "Read the pickled data back into Pandas and display the first 5 records. In this example, the pickled dataframe is serialized to \"ctgov.pckl\" in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acronym</th>\n",
       "      <th>arm_group.arm_group_label</th>\n",
       "      <th>arm_group.arm_group_type</th>\n",
       "      <th>arm_group.description</th>\n",
       "      <th>biospec_descr.textblock</th>\n",
       "      <th>biospec_retention</th>\n",
       "      <th>brief_summary.textblock</th>\n",
       "      <th>brief_title</th>\n",
       "      <th>clinical_results.baseline.group_list.group.description</th>\n",
       "      <th>clinical_results.baseline.group_list.group.title</th>\n",
       "      <th>...</th>\n",
       "      <th>sponsors.collaborator.agency</th>\n",
       "      <th>sponsors.collaborator.agency_class</th>\n",
       "      <th>sponsors.lead_sponsor.agency</th>\n",
       "      <th>sponsors.lead_sponsor.agency_class</th>\n",
       "      <th>start_date</th>\n",
       "      <th>study_design</th>\n",
       "      <th>study_type</th>\n",
       "      <th>target_duration</th>\n",
       "      <th>verification_date</th>\n",
       "      <th>why_stopped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACCORD</td>\n",
       "      <td>[Glycemia Trial: intensive control, Glycemia T...</td>\n",
       "      <td>[Experimental, Active Comparator, Experimental...</td>\n",
       "      <td>[Open label administration of oral anti-hyperg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The purpose of this study is to prevent major ...</td>\n",
       "      <td>Action to Control Cardiovascular Risk in Diabe...</td>\n",
       "      <td>[Open label administration of oral anti-hyperg...</td>\n",
       "      <td>[Glycemia Trial: Intensive Control, Glycemia T...</td>\n",
       "      <td>...</td>\n",
       "      <td>[National Institute of Diabetes and Digestive ...</td>\n",
       "      <td>[NIH, NIH, NIH, U.S. Fed]</td>\n",
       "      <td>National Heart, Lung, and Blood Institute (NHLBI)</td>\n",
       "      <td>NIH</td>\n",
       "      <td>September 1999</td>\n",
       "      <td>Allocation: Randomized, Endpoint Classificatio...</td>\n",
       "      <td>Interventional</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November 2014</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patients with Zollinger-Ellison Syndrome suffe...</td>\n",
       "      <td>Combination Chemotherapy in Patients With Zoll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National Institute of Diabetes and Digestive a...</td>\n",
       "      <td>NIH</td>\n",
       "      <td>September 1978</td>\n",
       "      <td>Endpoint Classification: Safety/Efficacy Study...</td>\n",
       "      <td>Interventional</td>\n",
       "      <td>NaN</td>\n",
       "      <td>August 2003</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patients with Zollinger-Ellison Syndrome suffe...</td>\n",
       "      <td>The Use of Oral Omeprazole and Intravenous Pan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National Institute of Diabetes and Digestive a...</td>\n",
       "      <td>NIH</td>\n",
       "      <td>February 1983</td>\n",
       "      <td>Primary Purpose: Treatment</td>\n",
       "      <td>Interventional</td>\n",
       "      <td>NaN</td>\n",
       "      <td>December 2007</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This study will examine the safety and effecti...</td>\n",
       "      <td>Interferon and Octreotide to Treat Zollinger-E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National Institute of Diabetes and Digestive a...</td>\n",
       "      <td>NIH</td>\n",
       "      <td>October 1988</td>\n",
       "      <td>Primary Purpose: Treatment</td>\n",
       "      <td>Interventional</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September 2007</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In patients with Zollinger-Ellison Syndrome th...</td>\n",
       "      <td>Treatment of Zollinger-Ellison Syndrome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National Institute of Diabetes and Digestive a...</td>\n",
       "      <td>NIH</td>\n",
       "      <td>January 1989</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Observational</td>\n",
       "      <td>NaN</td>\n",
       "      <td>December 2007</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  acronym                          arm_group.arm_group_label  \\\n",
       "0  ACCORD  [Glycemia Trial: intensive control, Glycemia T...   \n",
       "1     NaN                                                NaN   \n",
       "2     NaN                                                NaN   \n",
       "3     NaN                                                NaN   \n",
       "4     NaN                                                NaN   \n",
       "\n",
       "                            arm_group.arm_group_type  \\\n",
       "0  [Experimental, Active Comparator, Experimental...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                               arm_group.description biospec_descr.textblock  \\\n",
       "0  [Open label administration of oral anti-hyperg...                     NaN   \n",
       "1                                                NaN                     NaN   \n",
       "2                                                NaN                     NaN   \n",
       "3                                                NaN                     NaN   \n",
       "4                                                NaN                     NaN   \n",
       "\n",
       "  biospec_retention                            brief_summary.textblock  \\\n",
       "0               NaN  The purpose of this study is to prevent major ...   \n",
       "1               NaN  Patients with Zollinger-Ellison Syndrome suffe...   \n",
       "2               NaN  Patients with Zollinger-Ellison Syndrome suffe...   \n",
       "3               NaN  This study will examine the safety and effecti...   \n",
       "4               NaN  In patients with Zollinger-Ellison Syndrome th...   \n",
       "\n",
       "                                         brief_title  \\\n",
       "0  Action to Control Cardiovascular Risk in Diabe...   \n",
       "1  Combination Chemotherapy in Patients With Zoll...   \n",
       "2  The Use of Oral Omeprazole and Intravenous Pan...   \n",
       "3  Interferon and Octreotide to Treat Zollinger-E...   \n",
       "4            Treatment of Zollinger-Ellison Syndrome   \n",
       "\n",
       "  clinical_results.baseline.group_list.group.description  \\\n",
       "0  [Open label administration of oral anti-hyperg...       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                NaN       \n",
       "4                                                NaN       \n",
       "\n",
       "    clinical_results.baseline.group_list.group.title     ...      \\\n",
       "0  [Glycemia Trial: Intensive Control, Glycemia T...     ...       \n",
       "1                                                NaN     ...       \n",
       "2                                                NaN     ...       \n",
       "3                                                NaN     ...       \n",
       "4                                                NaN     ...       \n",
       "\n",
       "                        sponsors.collaborator.agency  \\\n",
       "0  [National Institute of Diabetes and Digestive ...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "  sponsors.collaborator.agency_class  \\\n",
       "0          [NIH, NIH, NIH, U.S. Fed]   \n",
       "1                                NaN   \n",
       "2                                NaN   \n",
       "3                                NaN   \n",
       "4                                NaN   \n",
       "\n",
       "                        sponsors.lead_sponsor.agency  \\\n",
       "0  National Heart, Lung, and Blood Institute (NHLBI)   \n",
       "1  National Institute of Diabetes and Digestive a...   \n",
       "2  National Institute of Diabetes and Digestive a...   \n",
       "3  National Institute of Diabetes and Digestive a...   \n",
       "4  National Institute of Diabetes and Digestive a...   \n",
       "\n",
       "  sponsors.lead_sponsor.agency_class      start_date  \\\n",
       "0                                NIH  September 1999   \n",
       "1                                NIH  September 1978   \n",
       "2                                NIH   February 1983   \n",
       "3                                NIH    October 1988   \n",
       "4                                NIH    January 1989   \n",
       "\n",
       "                                        study_design      study_type  \\\n",
       "0  Allocation: Randomized, Endpoint Classificatio...  Interventional   \n",
       "1  Endpoint Classification: Safety/Efficacy Study...  Interventional   \n",
       "2                         Primary Purpose: Treatment  Interventional   \n",
       "3                         Primary Purpose: Treatment  Interventional   \n",
       "4                                                N/A   Observational   \n",
       "\n",
       "  target_duration verification_date why_stopped  \n",
       "0             NaN     November 2014         NaN  \n",
       "1             NaN       August 2003         NaN  \n",
       "2             NaN     December 2007         NaN  \n",
       "3             NaN    September 2007         NaN  \n",
       "4             NaN     December 2007         NaN  \n",
       "\n",
       "[5 rows x 187 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "ctgov_data = pd.read_pickle(os.path.join(data_dir, 'ctgov.pckl'))\n",
    "ctgov_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract criteria\n",
    "\n",
    "Read in the serialized data from clinicaltrials.gov and extract inclusion/exlcusion criteria, one per row. Output a Series(id_info.nct_id, Criteria, Inclusion, TokenCount)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def id_generator(first_val=0, inc_func=lambda val: val + 1):\n",
    "    \"\"\"\n",
    "        Simple id generator. It takes first val & increase function and yields ids as needed.\n",
    "        Will return integers starting from 0 by default.\n",
    "    \"\"\"\n",
    "    id = first_val\n",
    "    while True:\n",
    "        yield id\n",
    "        id = inc_func(id)\n",
    "        \n",
    "        \n",
    "def __process_criteria(data, get_criteria_id):\n",
    "    \"\"\"\n",
    "        Extract inclusion and exclusion criteria from the clinical trials data.\n",
    "        Then - Tokenize and write the extracted data to a frame.\n",
    "    \"\"\"\n",
    "    pat = r\"^([\\w\\-]*\\s*){0,5}%s criteria[\\s\\w\\(\\),]*:\"\n",
    "    inpat = re.compile(pat % 'inclusion', re.UNICODE)\n",
    "    expat = re.compile(pat % 'exclusion', re.UNICODE)\n",
    "    try:\n",
    "        incl = True\n",
    "        nct_id = data[1]\n",
    "        txt = [_.strip() for _ in data[2].split(u'\\n\\n')]\n",
    "        for l in txt:\n",
    "            if re.match(inpat, l.lower()):\n",
    "                incl = True\n",
    "            elif re.match(expat, l.lower()):\n",
    "                incl = False\n",
    "            else:\n",
    "                toks = nltk.word_tokenize(l)\n",
    "                cri_id = next(get_criteria_id)\n",
    "                s = {'criteria_id': cri_id, 'NctId': nct_id, 'Criteria': unicode(l), 'Include': incl, 'Tokens': toks, 'TokenCount': len(toks)}\n",
    "                yield s\n",
    "    except Exception as e:\n",
    "        print(\"Error processing row %s: %s\" % (data[2], e))\n",
    "\n",
    "        \n",
    "def extract_criteria(data):\n",
    "    \"\"\"\n",
    "        Extract inclusion and exclusion criteria from each clinical trial into\n",
    "    \"\"\"\n",
    "    print(\"Transforming data (extracting criteria)\")\n",
    "    criteria_id_generator = id_generator()\n",
    "    transformed = [s for row in data[['id_info.nct_id', 'eligibility.criteria.textblock']].itertuples() for s in\n",
    "                   __process_criteria(row, criteria_id_generator)]\n",
    "    df = pd.DataFrame(transformed)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data and write the result to a file. (You'll notice that the script logs an error for one row. This is expected and results from that row being a \"NaN\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data (extracting criteria)\n",
      "Error processing row nan: 'float' object has no attribute 'split'\n",
      "Error processing row nan: 'float' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "ctgov_data = pd.read_pickle(os.path.join(data_dir, 'ctgov.pckl'))\n",
    "# Extract criteria\n",
    "criteria = extract_criteria(ctgov_data)\n",
    "criteria.to_pickle(os.path.join(data_dir, 'ct_criteria.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back the data and display a record selected by column value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Criteria</th>\n",
       "      <th>Include</th>\n",
       "      <th>NctId</th>\n",
       "      <th>TokenCount</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>criteria_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INCLUSION CRITERIA</td>\n",
       "      <td>True</td>\n",
       "      <td>NCT00001165</td>\n",
       "      <td>2</td>\n",
       "      <td>[INCLUSION, CRITERIA]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Criteria Include        NctId  TokenCount                 Tokens  \\\n",
       "4  INCLUSION CRITERIA    True  NCT00001165           2  [INCLUSION, CRITERIA]   \n",
       "\n",
       "   criteria_id  \n",
       "4            4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria = pd.read_pickle(os.path.join(data_dir, 'ct_criteria.pckl'))\n",
    "criteria.loc[criteria['criteria_id'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Criteria</th>\n",
       "      <th>Include</th>\n",
       "      <th>NctId</th>\n",
       "      <th>TokenCount</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>criteria_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Criteria, Include, NctId, TokenCount, Tokens, criteria_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria.loc[criteria['NctId'] == 'NCT01373190']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag, lemmatize, ngrammize\n",
    "\n",
    "Processes the extracted criteria with the NLTK POS tagger and lemmatizer and generates ngrams of 1-3 words (note: while unigrams are technically duplicated as 'Tokens', it will be more convenient to allow this and keep them in one column with bigrams and trigrams). Preprocesses the tokens by removing special characters and punctuation. Lemmata and ngrams are lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __lemmatise(lemmatizer, r):\n",
    "    wn_tags = {'NN': nltk.corpus.wordnet.NOUN, 'JJ': nltk.corpus.wordnet.ADJ, 'VB': nltk.corpus.wordnet.VERB,\n",
    "               'RB': nltk.corpus.wordnet.ADV}\n",
    "    return [(t[0], lemmatizer.lemmatize(t[0].lower(), pos=wn_tags.get(t[1][:2], nltk.corpus.wordnet.NOUN)).lower()) for\n",
    "            t in r]\n",
    "\n",
    "\n",
    "def tag_and_stem(data):\n",
    "    print(\"Transforming data (tagging and lemmatising)\")\n",
    "    series = []\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    punct = '[%s]*' % re.escape(string.punctuation)\n",
    "    pat = re.compile(r\"^(%(p)s[\\w\\d]+%(p)s)+$\" % {'p': punct}, re.UNICODE)\n",
    "    # Itertuples is 50% faster than df.apply()\n",
    "    for row in progressbar(data[['NctId', 'Tokens', 'criteria_id']].itertuples(), total=data.shape[0]):\n",
    "        nct_id = row[1]\n",
    "        toks = filter(lambda t: re.match(pat, t), row[2])\n",
    "        cri_id = row[3]\n",
    "        tags = nltk.pos_tag(toks)\n",
    "        lemmas = __lemmatise(lemmatizer, tags)\n",
    "        ngrams = []\n",
    "        for n in (1, 2, 3):\n",
    "            ngrams += list(nltk.ngrams([(lemma[1], tags[idx][1]) for idx, lemma in enumerate(lemmas)], n))\n",
    "        s = {'criteria_id': cri_id, 'NctId': nct_id, 'Tokens': toks, 'Tags': tags, 'Lemmas': lemmas, 'Ngrams': ngrams}\n",
    "        series.append(s)\n",
    "    df = pd.DataFrame(series)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the extracted criteria (stored in \"ct_criteria.pckl\" in the previous step), tag, lemmatize and ngrammize the data and store it as \"ct_tagged.pckl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/99946 [00:57<60:10:40,  0.46it/s]"
     ]
    }
   ],
   "source": [
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "\n",
    "criteria = pd.read_pickle(os.path.join(data_dir, 'ct_criteria.pckl'))\n",
    "tagged = tag_and_stem(criteria)\n",
    "tagged.to_pickle(os.path.join(data_dir, 'ct_tagged.pckl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged = pd.read_pickle(os.path.join(data_dir, 'ct_tagged.pckl'))\n",
    "tagged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter criteria\n",
    "Filters out criteria composed entirely of function words and stopwords. Strips ngrams composed entirely of stop words/tags from the ngram list. By default this function uses the NTLK stopword list and all PTB tags except nouns. Additional lists of stop words and stop tags can be supplied with keyword arguments (\"stop_words\", \"stop_tags\"). Returns a tuple of dataframes, (filtered_criteria, excluded_criteria).\n",
    "\n",
    "(Note: this step generates a SettingWithCopyWarning. This is known and is a false positive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __filter(values, idx, stops):\n",
    "    return not set([t[idx] for t in values]) <= stops\n",
    "\n",
    "\n",
    "def filter_criteria(data, user_stop_words=[], user_stop_tags=[]):\n",
    "    print(\"Filtering criteria\")\n",
    "    default_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    default_stop_tags = [\"$\", \"''\", \"(\", \")\", \",\", \"--\", \".\", \":\", \"CC\", \"CD\", \"DT\",\n",
    "                         \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\",\n",
    "                         \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\",\n",
    "                         \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\",\n",
    "                         \"WDT\", \"WP\", \"WP$\", \"WRB\", \"``\"]\n",
    "    print(\"Filtering stops\")\n",
    "    stop_words = set(default_stop_words + user_stop_words)\n",
    "    stop_tags = set(default_stop_tags + user_stop_tags)\n",
    "    excluded = pd.DataFrame()\n",
    "    for col, idx, stops in (\n",
    "            ('Lemmas', 0, stop_words),\n",
    "            ('Tags', 1, stop_tags)):  # Lemma filtering excludes 18 rows, tag filtering excludes 205\n",
    "        data['Ngrams'] = data['Ngrams'].apply(lambda row: [ngram for ngram in row if __filter(ngram, idx, stops)])\n",
    "        groups = data.groupby(lambda r: __filter(data[col].loc[r], 1, stops))\n",
    "        data = groups.get_group(True)\n",
    "        excluded = excluded.append(groups.get_group(False)) if groups.groups.has_key(False) else excluded\n",
    "    return (data, excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the tagged criteria (stored in \"ct_tagged.pckl\" in the previous step), filter out noise and write the results to \"ct_filtered.pckl\" (the included criteria) and \"ct_excluded.pckl\" (the excluded criteria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "\n",
    "criteria = pd.read_pickle(os.path.join(data_dir, 'ct_tagged.pckl'))\n",
    "incl, excl = filter_criteria(criteria)\n",
    "incl.to_pickle(os.path.join(data_dir, 'ct_filtered.pckl'))\n",
    "excl.to_pickle(os.path.join(data_dir, 'ct_excluded.pckl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove ngram duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = __mkdir(working_dir, \"data_diab\")\n",
    "incl = pd.read_pickle(os.path.join(data_dir, 'ct_filtered.pckl'))\n",
    "incl['Ngrams'] = incl['Ngrams'].apply(lambda ngrams: list(set(ngrams)))\n",
    "incl.to_pickle(os.path.join(data_dir, 'ct_filtered.pckl'))\n",
    "incl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excl = pd.read_pickle(os.path.join(data_dir, 'ct_excluded.pckl'))\n",
    "excl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
